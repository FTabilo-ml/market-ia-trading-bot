{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Agrega la raíz del proyecto a sys.path\n",
    "sys.path.append(str(pathlib.Path().resolve().parent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            net_buy  buy_count  sell_count  amount_net\n",
      "Date                                                  \n",
      "2020-03-19      0.0        0.0         0.0         NaN\n",
      "2020-03-20      1.0        1.0         0.0     15001.0\n",
      "2020-03-23      0.0        0.0         0.0         NaN\n",
      "2020-03-24      1.0        1.0         0.0     15001.0\n",
      "2020-03-25      0.0        0.0         0.0         NaN\n",
      "2020-03-26      0.0        0.0         0.0         NaN\n",
      "2020-03-27      0.0        0.0         0.0         NaN\n",
      "2020-03-30      0.0        0.0         0.0         NaN\n",
      "2020-03-31      0.0        0.0         0.0         NaN\n",
      "2020-04-01      0.0        0.0         0.0         NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"C:/Users/felip/market-ia-trading-bot/data/processed/AAPL.parquet\", engine=\"fastparquet\")\n",
    "print(df[[\"net_buy\",\"buy_count\",\"sell_count\",\"amount_net\"]].tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: c:\\Users\\felip\\market-ia-trading-bot\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Working dir:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SimFinId',\n",
       " 'Currency_x',\n",
       " 'Fiscal Year',\n",
       " 'Fiscal Period_x',\n",
       " 'Publish Date_x',\n",
       " 'Restated Date_x',\n",
       " 'Shares (Basic)_x',\n",
       " 'Shares (Diluted)_x',\n",
       " 'Revenue',\n",
       " 'Cost of Revenue',\n",
       " 'Gross Profit',\n",
       " 'Operating Expenses',\n",
       " 'Selling, General & Administrative',\n",
       " 'Research & Development',\n",
       " 'Depreciation & Amortization_x',\n",
       " 'Operating Income (Loss)',\n",
       " 'Non-Operating Income (Loss)',\n",
       " 'Interest Expense, Net',\n",
       " 'Pretax Income (Loss), Adj.',\n",
       " 'Abnormal Gains (Losses)',\n",
       " 'Pretax Income (Loss)',\n",
       " 'Income Tax (Expense) Benefit, Net',\n",
       " 'Income (Loss) from Continuing Operations',\n",
       " 'Net Extraordinary Gains (Losses)',\n",
       " 'Net Income',\n",
       " 'Net Income (Common)',\n",
       " 'Currency_y',\n",
       " 'Fiscal Period_y',\n",
       " 'Publish Date_y',\n",
       " 'Restated Date_y',\n",
       " 'Shares (Basic)_y',\n",
       " 'Shares (Diluted)_y',\n",
       " 'Cash, Cash Equivalents & Short Term Investments',\n",
       " 'Accounts & Notes Receivable',\n",
       " 'Inventories',\n",
       " 'Total Current Assets',\n",
       " 'Property, Plant & Equipment, Net',\n",
       " 'Long Term Investments & Receivables',\n",
       " 'Other Long Term Assets',\n",
       " 'Total Noncurrent Assets',\n",
       " 'Total Assets',\n",
       " 'Payables & Accruals',\n",
       " 'Short Term Debt',\n",
       " 'Total Current Liabilities',\n",
       " 'Long Term Debt',\n",
       " 'Total Noncurrent Liabilities',\n",
       " 'Total Liabilities',\n",
       " 'Share Capital & Additional Paid-In Capital',\n",
       " 'Treasury Stock',\n",
       " 'Retained Earnings',\n",
       " 'Total Equity',\n",
       " 'Total Liabilities & Equity',\n",
       " 'Currency',\n",
       " 'Fiscal Period',\n",
       " 'Publish Date',\n",
       " 'Restated Date',\n",
       " 'Shares (Basic)',\n",
       " 'Shares (Diluted)',\n",
       " 'Net Income/Starting Line',\n",
       " 'Depreciation & Amortization_y',\n",
       " 'Non-Cash Items',\n",
       " 'Change in Working Capital',\n",
       " 'Change in Accounts Receivable',\n",
       " 'Change in Inventories',\n",
       " 'Change in Accounts Payable',\n",
       " 'Change in Other',\n",
       " 'Net Cash from Operating Activities',\n",
       " 'Change in Fixed Assets & Intangibles',\n",
       " 'Net Change in Long Term Investment',\n",
       " 'Net Cash from Acquisitions & Divestitures',\n",
       " 'Net Cash from Investing Activities',\n",
       " 'Dividends Paid',\n",
       " 'Cash from (Repayment of) Debt',\n",
       " 'Cash from (Repurchase of) Equity',\n",
       " 'Net Cash from Financing Activities',\n",
       " 'Net Change in Cash']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'volume_adi',\n",
      "       'volume_obv', 'volume_cmf', 'volume_fi', 'volume_em', 'volume_sma_em',\n",
      "       'volume_vpt', 'volume_vwap', 'volume_mfi', 'volume_nvi',\n",
      "       'volatility_bbm', 'volatility_bbh', 'volatility_bbl', 'volatility_bbw'],\n",
      "      dtype='object')\n",
      "            daily_return  net_buy  sentiment_score\n",
      "Date                                              \n",
      "2020-03-26      0.052623      0.0              0.0\n",
      "2020-03-27     -0.041402      0.0              0.0\n",
      "2020-03-30      0.028538      0.0              0.0\n",
      "2020-03-31     -0.002041      0.0              0.0\n",
      "2020-04-01     -0.052617      0.0              0.0\n",
      "1980-12-12 00:00:00 2020-04-01 00:00:00 9909\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"C:/Users/felip/market-ia-trading-bot/data/processed/AAPL.parquet\", engine=\"fastparquet\")\n",
    "print(df.columns[:20])                    # mira qué columnas hay\n",
    "print(df[[\"daily_return\",\"net_buy\",\"sentiment_score\"]].tail(5))\n",
    "print(df.index.min(), df.index.max(), len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Literal, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.metrics import ndcg_score\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando DATA_ML: c:\\Users\\felip\\market-ia-trading-bot\\data\\ml\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start: Path = Path.cwd()) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"data\" / \"ml\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"No pude encontrar la carpeta data/ml subiendo por los padres.\")\n",
    "\n",
    "ROOT = find_repo_root()                 # ← auto-detecta la raíz del repo\n",
    "DATA_ML = ROOT / \"data\" / \"ml\"\n",
    "ARTS = ROOT / \"artifacts\" / \"rank\"\n",
    "ARTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "H = 36\n",
    "SPLIT = \"2017-01-01\"\n",
    "REL_BINS = 4\n",
    "EVAL_K = [5, 10]\n",
    "SIGN_FROM = \"val\"\n",
    "VAL_MONTHS = 24\n",
    "SIGN_THRESHOLD = 0.01\n",
    "SCORE_FUTURE = True\n",
    "\n",
    "dataset_fp = DATA_ML / f\"dataset_rank_{H}m.parquet\"\n",
    "print(\"Usando DATA_ML:\", DATA_ML)\n",
    "assert dataset_fp.exists(), f\"No existe {dataset_fp}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_key(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s).dt.to_period(\"M\").astype(str)\n",
    "\n",
    "def make_rel_labels_per_month(y: pd.Series, month: pd.Series, bins: int) -> np.ndarray:\n",
    "    if bins < 2:\n",
    "        raise ValueError(\"rel_bins debe ser >= 2\")\n",
    "    df = pd.DataFrame({\"y\": y.values, \"m\": month.values})\n",
    "    labels = np.full(len(df), np.nan, dtype=float)\n",
    "    for m, g in df.groupby(\"m\", sort=False):\n",
    "        vals = g[\"y\"].to_numpy(dtype=float)\n",
    "        if np.isnan(vals).all():\n",
    "            continue\n",
    "        valid = ~np.isnan(vals)\n",
    "        if valid.sum() == 0:\n",
    "            continue\n",
    "        ranks = np.argsort(np.argsort(vals[valid]))  # 0..n-1 ascendente\n",
    "        frac = (ranks + 1) / (valid.sum() + 1e-9)\n",
    "        lab = np.minimum(bins - 1, (frac * bins).astype(int))\n",
    "        tmp = np.full_like(vals, np.nan, dtype=float)\n",
    "        tmp[valid] = lab\n",
    "        labels[g.index.values] = tmp\n",
    "    out = labels\n",
    "    out = np.where(np.isnan(out), -1, out)  # no debería usarse en fit si -1\n",
    "    return out.astype(int)\n",
    "\n",
    "def groups_by_month(m: pd.Series) -> np.ndarray:\n",
    "    sizes = m.value_counts().sort_index()\n",
    "    return sizes.to_numpy(dtype=int)\n",
    "\n",
    "def monthly_ic(df: pd.DataFrame, score_col: str, y_col: str) -> Tuple[float, float, int, pd.DataFrame]:\n",
    "    det = (\n",
    "        df.groupby(\"month\", as_index=False)\n",
    "          .apply(lambda g: pd.Series({\"ic\": g[score_col].corr(g[y_col], method=\"spearman\")}))\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    det[\"ic\"] = det[\"ic\"].astype(float)\n",
    "    det = det.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"ic\"])\n",
    "    mean_ic = float(det[\"ic\"].mean()) if not det.empty else np.nan\n",
    "    std_ic = float(det[\"ic\"].std(ddof=0)) if len(det) > 1 else 0.0\n",
    "    return mean_ic, std_ic, int(len(det)), det\n",
    "\n",
    "def ndcg_by_month(df: pd.DataFrame, score_col: str, y_col: str, ks: List[int]) -> Dict[int, float]:\n",
    "    out: Dict[int, List[float]] = {k: [] for k in ks}\n",
    "    for m, g in df.groupby(\"month\"):\n",
    "        y_true = g[y_col].to_numpy(dtype=float).reshape(1, -1)\n",
    "        y_score = g[score_col].to_numpy(dtype=float).reshape(1, -1)\n",
    "        if np.isnan(y_true).any() or np.isnan(y_score).any():\n",
    "            continue\n",
    "        for k in ks:\n",
    "            k_eff = max(1, min(k, y_true.shape[1]))\n",
    "            out[k].append(float(ndcg_score(y_true, y_score, k=k_eff)))\n",
    "    return {k: (np.mean(v) if v else np.nan) for k, v in out.items()}\n",
    "\n",
    "def top_bottom_spread(df: pd.DataFrame, score_col: str, y_col: str, k: int) -> Tuple[float, float]:\n",
    "    spreads: List[float] = []\n",
    "    pos = 0\n",
    "    for m, g in df.groupby(\"month\"):\n",
    "        gg = g.sort_values(score_col, ascending=False)\n",
    "        k_eff = max(1, min(k, len(gg)))\n",
    "        top_y = float(gg.head(k_eff)[y_col].mean())\n",
    "        bot_y = float(gg.tail(k_eff)[y_col].mean())\n",
    "        sp = top_y - bot_y\n",
    "        spreads.append(sp)\n",
    "        if sp > 0:\n",
    "            pos += 1\n",
    "    if not spreads:\n",
    "        return np.nan, np.nan\n",
    "    return float(np.mean(spreads)), float(pos / len(spreads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_key(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s).dt.to_period(\"M\").astype(str)\n",
    "\n",
    "def make_rel_labels_per_month(y: pd.Series, month: pd.Series, bins: int) -> np.ndarray:\n",
    "    if bins < 2:\n",
    "        raise ValueError(\"rel_bins debe ser >= 2\")\n",
    "    df = pd.DataFrame({\"y\": y.values, \"m\": month.values})\n",
    "    labels = np.full(len(df), np.nan, dtype=float)\n",
    "    for m, g in df.groupby(\"m\", sort=False):\n",
    "        vals = g[\"y\"].to_numpy(dtype=float)\n",
    "        if np.isnan(vals).all():\n",
    "            continue\n",
    "        valid = ~np.isnan(vals)\n",
    "        if valid.sum() == 0:\n",
    "            continue\n",
    "        ranks = np.argsort(np.argsort(vals[valid]))  # 0..n-1 ascendente\n",
    "        frac = (ranks + 1) / (valid.sum() + 1e-9)\n",
    "        lab = np.minimum(bins - 1, (frac * bins).astype(int))\n",
    "        tmp = np.full_like(vals, np.nan, dtype=float)\n",
    "        tmp[valid] = lab\n",
    "        labels[g.index.values] = tmp\n",
    "    out = labels\n",
    "    out = np.where(np.isnan(out), -1, out)  # no debería usarse en fit si -1\n",
    "    return out.astype(int)\n",
    "\n",
    "def groups_by_month(m: pd.Series) -> np.ndarray:\n",
    "    sizes = m.value_counts().sort_index()\n",
    "    return sizes.to_numpy(dtype=int)\n",
    "\n",
    "def monthly_ic(df: pd.DataFrame, score_col: str, y_col: str) -> Tuple[float, float, int, pd.DataFrame]:\n",
    "    det = (\n",
    "        df.groupby(\"month\", as_index=False)\n",
    "          .apply(lambda g: pd.Series({\"ic\": g[score_col].corr(g[y_col], method=\"spearman\")}))\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    det[\"ic\"] = det[\"ic\"].astype(float)\n",
    "    det = det.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"ic\"])\n",
    "    mean_ic = float(det[\"ic\"].mean()) if not det.empty else np.nan\n",
    "    std_ic = float(det[\"ic\"].std(ddof=0)) if len(det) > 1 else 0.0\n",
    "    return mean_ic, std_ic, int(len(det)), det\n",
    "\n",
    "def ndcg_by_month(df: pd.DataFrame, score_col: str, y_col: str, ks: List[int]) -> Dict[int, float]:\n",
    "    out: Dict[int, List[float]] = {k: [] for k in ks}\n",
    "    for m, g in df.groupby(\"month\"):\n",
    "        y_true = g[y_col].to_numpy(dtype=float).reshape(1, -1)\n",
    "        y_score = g[score_col].to_numpy(dtype=float).reshape(1, -1)\n",
    "        if np.isnan(y_true).any() or np.isnan(y_score).any():\n",
    "            continue\n",
    "        for k in ks:\n",
    "            k_eff = max(1, min(k, y_true.shape[1]))\n",
    "            out[k].append(float(ndcg_score(y_true, y_score, k=k_eff)))\n",
    "    return {k: (np.mean(v) if v else np.nan) for k, v in out.items()}\n",
    "\n",
    "def top_bottom_spread(df: pd.DataFrame, score_col: str, y_col: str, k: int) -> Tuple[float, float]:\n",
    "    spreads: List[float] = []\n",
    "    pos = 0\n",
    "    for m, g in df.groupby(\"month\"):\n",
    "        gg = g.sort_values(score_col, ascending=False)\n",
    "        k_eff = max(1, min(k, len(gg)))\n",
    "        top_y = float(gg.head(k_eff)[y_col].mean())\n",
    "        bot_y = float(gg.tail(k_eff)[y_col].mean())\n",
    "        sp = top_y - bot_y\n",
    "        spreads.append(sp)\n",
    "        if sp > 0:\n",
    "            pos += 1\n",
    "    if not spreads:\n",
    "        return np.nan, np.nan\n",
    "    return float(np.mean(spreads)), float(pos / len(spreads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=292,704 | test=8,948 | future=84,631\n",
      "Rangos: train 1973-05-31..2016-12-31 | test 2017-01-31..2017-04-30\n"
     ]
    }
   ],
   "source": [
    "y_col = f\"y_fwd_{H}m\"\n",
    "df = pd.read_parquet(dataset_fp)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "df[\"month\"] = pd.to_datetime(df[\"Date\"]).dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# split\n",
    "df[\"split\"] = \"train\"\n",
    "df.loc[(df[\"Date\"] >= pd.to_datetime(SPLIT)) & (df[y_col].notna()), \"split\"] = \"test\"\n",
    "df.loc[(df[\"Date\"] >= pd.to_datetime(SPLIT)) & (df[y_col].isna()), \"split\"] = \"future\"\n",
    "\n",
    "tr = df[(df[\"split\"] == \"train\") & df[y_col].notna()].copy()\n",
    "te = df[(df[\"split\"] == \"test\") & df[y_col].notna()].copy()\n",
    "fu = df[(df[\"split\"] == \"future\")].copy()\n",
    "\n",
    "# muy importante: ordenar por month para que 'group' coincida con el orden de las filas\n",
    "tr = tr.sort_values([\"month\", \"Ticker\"]).reset_index(drop=True)\n",
    "te = te.sort_values([\"month\", \"Ticker\"]).reset_index(drop=True)\n",
    "fu = fu.sort_values([\"month\", \"Ticker\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"train={len(tr):,} | test={len(te):,} | future={len(fu):,}\")\n",
    "print(\"Rangos:\",\n",
    "      f\"train {tr['Date'].min().date()}..{tr['Date'].max().date()} |\",\n",
    "      f\"test {te['Date'].min().date() if not te.empty else 'NA'}..{te['Date'].max().date() if not te.empty else 'NA'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = {\"Date\", \"Ticker\", y_col, \"split\", \"month\"}\n",
    "numeric_cols = [c for c in df.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(df[c])]\n",
    "categorical_cols = [c for c in [\"Ticker\"] if c in df.columns]\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rel_labels_per_month(y: pd.Series, month: pd.Series, bins: int) -> np.ndarray:\n",
    "    if bins < 2:\n",
    "        raise ValueError(\"rel_bins debe ser >= 2\")\n",
    "    df_ = pd.DataFrame({\"y\": y.values, \"m\": month.values})\n",
    "    out = np.full(len(df_), np.nan, dtype=float)\n",
    "    for m, g in df_.groupby(\"m\", sort=False):\n",
    "        vals = g[\"y\"].to_numpy(dtype=float)\n",
    "        valid = ~np.isnan(vals)\n",
    "        if valid.sum() == 0:\n",
    "            continue\n",
    "        ranks = np.argsort(np.argsort(vals[valid]))\n",
    "        frac = (ranks + 1) / (valid.sum() + 1e-9)\n",
    "        lab = np.minimum(REL_BINS - 1, (frac * REL_BINS).astype(int))\n",
    "        tmp = np.full_like(vals, np.nan, dtype=float)\n",
    "        tmp[valid] = lab\n",
    "        out[g.index.values] = tmp\n",
    "    out = np.where(np.isnan(out), -1, out)\n",
    "    return out.astype(int)\n",
    "\n",
    "def groups_from_sorted_month(series_month: pd.Series) -> np.ndarray:\n",
    "    # asume que el DataFrame YA está ordenado por 'month'\n",
    "    return series_month.value_counts(sort=False).reindex(series_month.drop_duplicates(), fill_value=0).to_numpy()\n",
    "\n",
    "tr[\"rel_label\"] = make_rel_labels_per_month(tr[y_col], tr[\"month\"], REL_BINS)\n",
    "te[\"rel_label\"] = make_rel_labels_per_month(te[y_col], te[\"month\"], REL_BINS)\n",
    "\n",
    "g_tr = groups_from_sorted_month(tr[\"month\"])\n",
    "g_te = groups_from_sorted_month(te[\"month\"]) if not te.empty else None\n",
    "\n",
    "# chequeos\n",
    "assert g_tr.sum() == len(tr), \"group train no cuadra con filas\"\n",
    "if g_te is not None:\n",
    "    assert g_te.sum() == len(te), \"group test no cuadra con filas\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\lightgbm\\sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 292704, number of used features: 2469\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "\n",
    "# ganancias por bin (peso a la parte alta)\n",
    "gains = [0] + [int(round(2 ** (i - 1))) for i in range(1, REL_BINS)]\n",
    "\n",
    "model = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    random_state=42,\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    metric=\"ndcg\",\n",
    "    eval_at=[int(k) for k in EVAL_K],\n",
    "    label_gain=gains,\n",
    "    n_jobs=-1,\n",
    "    # regularización más fuerte si quieres:\n",
    "    # num_leaves=31, min_data_in_leaf=100, reg_lambda=1.0, reg_alpha=0.0\n",
    ")\n",
    "\n",
    "# 1) fit del preprocessor en TRAIN\n",
    "Xtr = tr[numeric_cols + categorical_cols]\n",
    "pre.fit(Xtr)\n",
    "\n",
    "# 2) transformar TRAIN y (si existe) TEST\n",
    "Xtr_t = pre.transform(Xtr)\n",
    "ytr = tr[\"rel_label\"].astype(int).values\n",
    "\n",
    "eval_sets = None\n",
    "eval_groups = None\n",
    "if not te.empty:\n",
    "    Xte = te[numeric_cols + categorical_cols]\n",
    "    Xte_t = pre.transform(Xte)     # <--- transformado\n",
    "    yte = te[\"rel_label\"].astype(int).values\n",
    "    eval_sets = [(Xte_t, yte)]\n",
    "    eval_groups = [g_te]\n",
    "\n",
    "# 3) entrenar el ranker con grupos\n",
    "if eval_sets is not None:\n",
    "    model.fit(Xtr_t, ytr, group=g_tr, eval_set=eval_sets, eval_group=eval_groups)\n",
    "else:\n",
    "    model.fit(Xtr_t, ytr, group=g_tr)\n",
    "\n",
    "# 4) construir un Pipeline \"ya entrenado\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([(\"prep\", pre), (\"model\", model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\lightgbm\\sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\lightgbm\\sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\lightgbm\\sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    }
   ],
   "source": [
    "# predicciones\n",
    "tr[\"raw_score\"] = pipe.predict(tr[numeric_cols + categorical_cols]).astype(float)\n",
    "te[\"raw_score\"] = pipe.predict(te[numeric_cols + categorical_cols]).astype(float)\n",
    "if SCORE_FUTURE and not fu.empty:\n",
    "    fu[\"raw_score\"] = pipe.predict(fu[numeric_cols + categorical_cols]).astype(float)\n",
    "\n",
    "# ... luego tu celda de chequeo de signo (IC) y flip ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIGN CHECK=val] IC mean=0.1876, std=0.0215, n_months=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_3904\\4159941757.py:33: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\"ic\": g[score_col].corr(g[y_col], method=\"spearman\")}))\n"
     ]
    }
   ],
   "source": [
    "if SIGN_FROM == \"val\":\n",
    "    months_tr = sorted(tr[\"month\"].unique())\n",
    "    val_set = set(months_tr[-max(1, VAL_MONTHS):])\n",
    "    chk = tr[tr[\"month\"].isin(val_set)].copy()\n",
    "else:\n",
    "    chk = te.copy()\n",
    "\n",
    "direction = +1\n",
    "ic_mean = np.nan\n",
    "ic_std = np.nan\n",
    "ic_n = 0\n",
    "if not chk.empty:\n",
    "    chk[\"score_tmp\"] = chk[\"raw_score\"]\n",
    "    ic_mean, ic_std, ic_n, det = monthly_ic(chk, \"score_tmp\", y_col)\n",
    "    print(f\"[SIGN CHECK={SIGN_FROM}] IC mean={ic_mean:.4f}, std={ic_std:.4f}, n_months={ic_n}\")\n",
    "    if np.isfinite(ic_mean) and ic_mean < -abs(float(SIGN_THRESHOLD)):\n",
    "        direction = -1\n",
    "        print(\"→ Señal invertida (flip) por IC medio negativo.\")\n",
    "\n",
    "for fr in (tr, te, fu):\n",
    "    if not fr.empty:\n",
    "        fr[\"score\"] = direction * fr[\"raw_score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC mensual (test) — mean=0.0542, std=0.0600, n=4\n",
      "NDCG@K (test): {5: 0.11668324787776539, 10: 0.12663317324293194}\n",
      "Spread Top5-Bottom5 — mean=1.4657, hit%=100.0%\n",
      "Spread Top10-Bottom10 — mean=1.1101, hit%=100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_3904\\4159941757.py:33: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\"ic\": g[score_col].corr(g[y_col], method=\"spearman\")}))\n",
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1807: FutureWarning: ndcg_score should not be used on negative y_true values. ndcg_score will raise a ValueError on negative y_true values starting from version 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1807: FutureWarning: ndcg_score should not be used on negative y_true values. ndcg_score will raise a ValueError on negative y_true values starting from version 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1807: FutureWarning: ndcg_score should not be used on negative y_true values. ndcg_score will raise a ValueError on negative y_true values starting from version 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1807: FutureWarning: ndcg_score should not be used on negative y_true values. ndcg_score will raise a ValueError on negative y_true values starting from version 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1807: FutureWarning: ndcg_score should not be used on negative y_true values. ndcg_score will raise a ValueError on negative y_true values starting from version 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1807: FutureWarning: ndcg_score should not be used on negative y_true values. ndcg_score will raise a ValueError on negative y_true values starting from version 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1807: FutureWarning: ndcg_score should not be used on negative y_true values. ndcg_score will raise a ValueError on negative y_true values starting from version 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\felip\\anaconda3\\envs\\market-ia-trading-bot\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1807: FutureWarning: ndcg_score should not be used on negative y_true values. ndcg_score will raise a ValueError on negative y_true values starting from version 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if not te.empty:\n",
    "    ic_mean_te, ic_std_te, ic_n_te, _ = monthly_ic(te, \"score\", y_col)\n",
    "    ndcg_te = ndcg_by_month(te, \"score\", y_col, EVAL_K)\n",
    "    print(f\"IC mensual (test) — mean={ic_mean_te:.4f}, std={ic_std_te:.4f}, n={ic_n_te}\")\n",
    "    print(\"NDCG@K (test):\", {k: float(v) for k,v in ndcg_te.items()})\n",
    "\n",
    "    for k in EVAL_K:\n",
    "        sp, pos = top_bottom_spread(te, \"score\", y_col, k)\n",
    "        print(f\"Spread Top{k}-Bottom{k} — mean={sp:.4f}, hit%={pos:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo guardado en: c:\\Users\\felip\\market-ia-trading-bot\\artifacts\\rank\\ranker_36m_lgbm.pkl\n",
      "✅ Predicciones guardadas en: c:\\Users\\felip\\market-ia-trading-bot\\data\\ml\\preds_rank_36m.parquet | filas: 386283\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "model_fp = ARTS / f\"ranker_{H}m_lgbm.pkl\"\n",
    "joblib.dump(pipe, model_fp)\n",
    "print(\"✅ Modelo guardado en:\", model_fp)\n",
    "\n",
    "# Predicciones\n",
    "preds = pd.concat([\n",
    "    tr[[\"Date\", \"Ticker\", \"split\", \"raw_score\", \"score\"]],\n",
    "    te[[\"Date\", \"Ticker\", \"split\", \"raw_score\", \"score\"]],\n",
    "    fu[[\"Date\", \"Ticker\", \"split\", \"raw_score\", \"score\"]],\n",
    "], ignore_index=True).sort_values([\"Date\",\"Ticker\"])\n",
    "\n",
    "out_fp = DATA_ML / f\"preds_rank_{H}m.parquet\"\n",
    "preds.to_parquet(out_fp, index=False)\n",
    "print(\"✅ Predicciones guardadas en:\", out_fp, \"| filas:\", len(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-ia-trading-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
